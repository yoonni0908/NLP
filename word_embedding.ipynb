{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdZnDI8azinw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40922c61-858d-4d5b-ea8b-cbdc9e7dafc3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANjD8c2VQRM_",
        "outputId": "8246b7d5-ad76-4ba5-dbb4-9c739e6970c0"
      },
      "source": [
        "# Mecab 인스톨\n",
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Counting objects: 100% (91/91), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 91 (delta 43), reused 22 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (91/91), done.\n",
            "/content/Mecab-ko-for-Google-Colab\n",
            "Installing konlpy.....\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 146kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 45.2MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 14.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: JPype1, beautifulsoup4, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2021-05-16 06:05:29--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22cd:e0db, 2406:da00:ff00::22c2:513, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=0Kc8sFQPKaovgzqxVunKrXdyn%2FE%3D&Expires=1621146929&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-05-16 06:05:29--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=0Kc8sFQPKaovgzqxVunKrXdyn%2FE%3D&Expires=1621146929&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.85.212\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.85.212|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-05-16 06:05:29 (22.4 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2021-05-16 06:06:42--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c2:513, 2406:da00:ff00::22cd:e0db, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=IPD3zdcfwEfqA7Q%2FpkGjj0v7qRk%3D&Expires=1621147002&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-05-16 06:06:42--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=IPD3zdcfwEfqA7Q%2FpkGjj0v7qRk%3D&Expires=1621147002&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.72.108\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.72.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M   106MB/s    in 0.4s    \n",
            "\n",
            "2021-05-16 06:06:43 (106 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "apt-get update\n",
            "apt-get upgrade\n",
            "apt install curl\n",
            "apt install git\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
            "Done\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n",
            "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n",
            "NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n",
            "블로그에 해결 방법을 남겨주신 tana님 감사합니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8njZiU_y1VI",
        "outputId": "22fc67be-2fc3-440e-ec9d-d24a7be283b9"
      },
      "source": [
        "import torch\n",
        "\n",
        "NLP = torch.FloatTensor([0,1,0,0,0,0,0,1])\n",
        "learning = torch.FloatTensor([0,0,0,1,0,0,0,1])\n",
        "\n",
        "print(torch.cosine_similarity(NLP, learning, dim=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.5000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-Yj5-JyNCLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c916d14-13c9-4c99-d27b-088a04493db2"
      },
      "source": [
        "from konlpy.tag import Mecab\n",
        "\n",
        "m = Mecab()\n",
        "res = m.pos(\"기계학습을 위한 텍스트 처리 방법을 공부해보자.\")\n",
        "print(res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('기계', 'NNG'), ('학습', 'NNG'), ('을', 'JKO'), ('위한', 'VV+ETM'), ('텍스트', 'NNG'), ('처리', 'NNG'), ('방법', 'NNG'), ('을', 'JKO'), ('공부', 'NNG'), ('해', 'XSV+EC'), ('보', 'VX'), ('자', 'EF'), ('.', 'SF')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKPBvjGsSTzm",
        "outputId": "ab195523-7680-4c05-c594-845c591d58e3"
      },
      "source": [
        "# 사전만들기 make_dic 함수수\n",
        "from konlpy.tag import Mecab\n",
        "from nltk import FreqDist\n",
        "import numpy as np\n",
        "\n",
        "def make_dic(texts):\n",
        "\n",
        "  words = []    # 단어 이곳에 저장\n",
        "  m = Mecab()\n",
        "\n",
        "  for snt in texts: # snt에 문장이 들어감\n",
        "    res = m.pos(snt) # 형태소 분석, 품사가 무엇인지 부착\n",
        "    # print(res)\n",
        "    for (lex,pos) in res:\n",
        "      if pos[0] == 'N' or pos[0] == 'V': #하늘 - N, 을 - J .. 약 45개의 품사, V는 동사, 시작이 N이면 체언\n",
        "        words.append(lex)\n",
        "\n",
        "  # 단어의 빈도수 계산\n",
        "  dic = FreqDist(np.hstack(words))\n",
        "  print('사전 크기: {0:d}'.format(len(dic)))\n",
        "\n",
        "  # 인덱스를 저장할 변수 초기화\n",
        "  indexes = {}\n",
        "  words = {}\n",
        "\n",
        "  # 단어에 고유 번호(인덱스) 부여\n",
        "  for num, word in enumerate(dic): # dictionary에서 하나씩 가져오기 위해 enumerate() 사용용\n",
        "    idx = num+2 # +2 중요함\n",
        "    indexes[word]= idx # word를 인덱스로\n",
        "    words[idx] = word #인덱스를 word로로\n",
        "\n",
        "  # 문장 길이 정규화에 사용한 패딩 인덱스\n",
        "  indexes['pad'] = 1 # 빈곳에 pad로 채움 -- 1번이 pad\n",
        "  words[1] = 'pad' \n",
        "  # 사전에 없는 단어에 대한 인덱스\n",
        "  indexes['unk'] = 0\n",
        "  words[0] = 'unk'\n",
        " \n",
        "  return (indexes, words)\n",
        "\n",
        "def word2index(indexes, word):\n",
        "  idx = indexes[word] if word in indexes else indexes['unk'] # index에 word값이 있으면 뽑아주고 없으면 unknown\n",
        "  return idx\n",
        "\n",
        "def index2word(words, index):\n",
        "  w = words[index] if index in words else -1\n",
        "  return w\n",
        "\n",
        "# corpus\n",
        "texts = [\"죽는 날까지 하늘을 우러러 한 점 부끄럼이 없기를\",\n",
        "         \"잎새에 이는 바람에도 나는 괴로워 했다.\",\n",
        "         \"별을 노래하는 마음으로 모든 죽어 가는 것을 사랑해야지\",\n",
        "         \"그리고 나한테 주어진 길을 걸어가야겠다.\",\n",
        "         \"오늘 밤에도 별이 바람에 스치운다.\"]\n",
        "\n",
        "(indexes, words) = make_dic(texts)\n",
        " \n",
        "print(word2index(indexes,'하늘'), word2index(indexes,'학교')) # '학교' 단어는 없으므로 0으로 출력됨\n",
        "print(index2word(words,4), index2word(words,0)) # unk는 없는 단어(out of vocabulary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "사전 크기: 24\n",
            "4 0\n",
            "하늘 unk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeTswZzN5YyU",
        "outputId": "7aab4f59-821d-46f2-d07c-0cbe9ebfe40b"
      },
      "source": [
        "# Word2Vec 사용하기 \n",
        "# 학습 데이터 만들기\n",
        "from konlpy.tag import Mecab\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "\n",
        "texts = [\"죽는 날까지 하늘을 우러러 한 점 부끄럼이 없기를\",\n",
        "         \"잎새에 이는 바람에도 나는 괴로워 했다.\",\n",
        "         \"별을 노래하는 마음으로 모든 죽어 가는 것을 사랑해야지\",\n",
        "         \"그리고 나한테 주어진 길을 걸어가야겠다.\",\n",
        "         \"오늘 밤에도 별이 바람에 스치운다.\"]\n",
        "         #주어진 corpus에서 형태소 분석석\n",
        "\n",
        "# Word2Vec 학습에 사용할 데이터 만들기\n",
        "\n",
        "m = Mecab()\n",
        "result = []\n",
        "\n",
        "for sent in texts:\n",
        "  tag = m.pos(sent)\n",
        "  words = []\n",
        "  for (lex,pos) in tag:\n",
        "    if pos[0] == 'N': # 명사류(체언)만 추출\n",
        "      words.append(lex)\n",
        "  result.append(words)\n",
        "\n",
        "print(result,\"\\n\") #[['날', '하늘', '점', '부끄럼'], ['잎새', 바람, '나']] => [['별', '노래', '마음', ... ]\n",
        "\n",
        "# Word2Vec 학습시키기\n",
        "model = Word2Vec(sentences=result, size=10, window=1, min_count=1, workers=1, sg=0)\n",
        "#size: embedding 차원 - 한단어를 10차원으로 , window: 컨텍스트 윈도우 크기, min_count: 단어 최소 빈도 수 제한 , workers: 학습을 위한 프로세스 수, sg : 0(CBOW), 1(Skip-gram)\n",
        "\n",
        "# 값 읽어오기\n",
        "print(model.wv['하늘']) #심볼을 숫자화 -> word embedding /wv : word to vector\n",
        "# 유사한 단어 가져오기\n",
        "print(model.wv.most_similar(\"하늘\"),\"\\n\") # cosine similarity해서 하늘과 유사한 다른 단어 뽑아옴\n",
        "\n",
        "# Word2Vec 모델 저장하기\n",
        "model.wv.save_word2vec_format('/gdrive/My Drive/colab/text_rep/test_w2v')\n",
        "\n",
        "# Word2Vec 모델 로드하기\n",
        "loaded_model = KeyedVectors.load_word2vec_format(\"/gdrive/My Drive/colab/text_rep/test_w2v\")\n",
        "\n",
        "# 값 읽어오기\n",
        "print(loaded_model.wv['하늘'])\n",
        "# 유사한 단어 가져오기\n",
        "print(loaded_model.wv.most_similar(\"하늘\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['날', '하늘', '점', '부끄럼'], ['잎새', '바람', '나'], ['별', '노래', '마음', '것', '사랑'], ['나', '길'], ['밤', '별', '바람']] \n",
            "\n",
            "[-0.00181364  0.01349637  0.0494625  -0.04351401 -0.0074907  -0.00855643\n",
            "  0.01056736 -0.04086038  0.0170731   0.01116189]\n",
            "[('날', 0.2550535202026367), ('사랑', 0.19656619429588318), ('바람', 0.18432015180587769), ('노래', 0.14699342846870422), ('밤', -0.058490533381700516), ('마음', -0.08099609613418579), ('것', -0.0850621908903122), ('점', -0.15721188485622406), ('별', -0.16460390388965607), ('잎새', -0.26042240858078003)] \n",
            "\n",
            "[-0.00181364  0.01349637  0.0494625  -0.04351401 -0.0074907  -0.00855643\n",
            "  0.01056736 -0.04086038  0.0170731   0.01116189]\n",
            "[('날', 0.2550535202026367), ('사랑', 0.19656619429588318), ('바람', 0.18432015180587769), ('노래', 0.14699342846870422), ('밤', -0.058490533381700516), ('마음', -0.08099609613418579), ('것', -0.0850621908903122), ('점', -0.15721188485622406), ('별', -0.16460390388965607), ('잎새', -0.26042240858078003)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZmC8MFZELmp",
        "outputId": "a73f12e6-e225-40b1-8c42-f3fdf5e22685"
      },
      "source": [
        "# GloVe 사용하기\n",
        "# GloVe 인스톨톨\n",
        "# colab 환경에 glove 설치\n",
        "! pip install glove-python-binary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting glove-python-binary\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/11/d8510a80110f736822856db566341dd2e1e7c3af536f77e409a6c09e0c22/glove_python_binary-0.2.0-cp37-cp37m-manylinux1_x86_64.whl (948kB)\n",
            "\r\u001b[K     |▍                               | 10kB 20.1MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 26.9MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 31.9MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40kB 28.5MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51kB 29.4MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 31.8MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71kB 28.5MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81kB 29.4MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 31.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 102kB 29.2MB/s eta 0:00:01\r\u001b[K     |███▉                            | 112kB 29.2MB/s eta 0:00:01\r\u001b[K     |████▏                           | 122kB 29.2MB/s eta 0:00:01\r\u001b[K     |████▌                           | 133kB 29.2MB/s eta 0:00:01\r\u001b[K     |████▉                           | 143kB 29.2MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 153kB 29.2MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 163kB 29.2MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 174kB 29.2MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 184kB 29.2MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 194kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████                         | 204kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 215kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 225kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 235kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 245kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 256kB 29.2MB/s eta 0:00:01\r\u001b[K     |█████████                       | 266kB 29.2MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 276kB 29.2MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 286kB 29.2MB/s eta 0:00:01\r\u001b[K     |██████████                      | 296kB 29.2MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 307kB 29.2MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 317kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 327kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 337kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 348kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 358kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 368kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 378kB 29.2MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 389kB 29.2MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 399kB 29.2MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 409kB 29.2MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 419kB 29.2MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 430kB 29.2MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 440kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 450kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 460kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 471kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 481kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 491kB 29.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 501kB 29.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 512kB 29.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 522kB 29.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 532kB 29.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 542kB 29.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 552kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 563kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 573kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 583kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 593kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 604kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 614kB 29.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 624kB 29.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 634kB 29.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 645kB 29.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 655kB 29.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 665kB 29.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 675kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 686kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 696kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 706kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 716kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 727kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 737kB 29.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 747kB 29.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 757kB 29.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 768kB 29.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 778kB 29.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 788kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 798kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 808kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 819kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 829kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 839kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 849kB 29.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 860kB 29.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 870kB 29.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 880kB 29.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 890kB 29.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 901kB 29.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 911kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 921kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 931kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 942kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 952kB 29.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove-python-binary) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove-python-binary) (1.19.5)\n",
            "Installing collected packages: glove-python-binary\n",
            "Successfully installed glove-python-binary-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLqHcIyLFIS5",
        "outputId": "134655f0-64f1-4d50-d25b-3975e17754de"
      },
      "source": [
        "from konlpy.tag import Mecab\n",
        "import numpy as np\n",
        "from glove import Corpus, Glove\n",
        "\n",
        "texts = [\"죽는 날까지 하늘을 우러러 한 점 부끄럼이 없기를\",\n",
        "         \"잎새에 이는 바람에도 나는 괴로워 했다.\",\n",
        "         \"별을 노래하는 마음으로 모든 죽어 가는 것을 사랑해야지\",\n",
        "         \"그리고 나한테 주어진 길을 걸어가야겠다.\",\n",
        "         \"오늘 밤에도 별이 바람에 스치운다.\"]\n",
        "\n",
        "# GloVe 학습에 사용할 데이터 만들기\n",
        "\n",
        "m = Mecab()\n",
        "result = []\n",
        "\n",
        "for sent in texts:\n",
        "  tag = m.pos(sent)\n",
        "  words = []\n",
        "  for (lex,pos) in tag:\n",
        "    if pos[0] == 'N':\n",
        "      words.append(lex)\n",
        "  result.append(words)\n",
        "\n",
        "print(result,\"\\n\")\n",
        "\n",
        "# 학습 및 평가가\n",
        "# Co-Occurrence Matrix 생성\n",
        "corpus = Corpus() \n",
        "corpus.fit(result, window=1) #corpus에 값을 채움, window=1 한단어 떨어져있는 것까지만 카운팅팅\n",
        "\n",
        "# GloVe 학습시키기\n",
        "glove = Glove(no_components=10, learning_rate=0.05)\n",
        "glove.fit(corpus.matrix, epochs=20, no_threads=1, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)\n",
        "\n",
        "# 값 읽어오기\n",
        "print(glove.word_vectors[glove.dictionary['하늘']]) # id를 넣으면 word_vector를 가져옴\n",
        "# 유사한 단어 가져오기\n",
        "print(glove.most_similar(\"하늘\"),\"\\n\")\n",
        "\n",
        "# GloVe 모델 저장하기\n",
        "glove.save('/gdrive/My Drive/colab/text_rep/test_glove')\n",
        "\n",
        "# GloVe 모델 로드하기\n",
        "loaded_model = glove.load(\"/gdrive/My Drive/colab/text_rep/test_glove\")\n",
        "\n",
        "# 값 읽어오기\n",
        "print(loaded_model.word_vectors[glove.dictionary['하늘']]) # word2index와 같은 기능\n",
        "# 유사한 단어 가져오기\n",
        "print(loaded_model.most_similar(\"하늘\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['날', '하늘', '점', '부끄럼'], ['잎새', '바람', '나'], ['별', '노래', '마음', '것', '사랑'], ['나', '길'], ['밤', '별', '바람']] \n",
            "\n",
            "Performing 20 training epochs with 1 threads\n",
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n",
            "Epoch 10\n",
            "Epoch 11\n",
            "Epoch 12\n",
            "Epoch 13\n",
            "Epoch 14\n",
            "Epoch 15\n",
            "Epoch 16\n",
            "Epoch 17\n",
            "Epoch 18\n",
            "Epoch 19\n",
            "[ 0.02104036 -0.01404578 -0.04851466  0.01803728 -0.01523357 -0.04068209\n",
            " -0.04581227 -0.04701197 -0.00781063 -0.01010458]\n",
            "[('부끄럼', 0.6554835514510583), ('점', 0.5460033054824257), ('나', 0.3306888587044728), ('날', 0.30147415271006095)] \n",
            "\n",
            "[ 0.02104036 -0.01404578 -0.04851466  0.01803728 -0.01523357 -0.04068209\n",
            " -0.04581227 -0.04701197 -0.00781063 -0.01010458]\n",
            "[('부끄럼', 0.6554835514510583), ('점', 0.5460033054824257), ('나', 0.3306888587044728), ('날', 0.30147415271006095)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fni57myGQpjO",
        "outputId": "3b167c31-26da-48e3-e2b6-e03ae9148480"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "train_data = 'I like deep learning I like NLP I enjoy flying'\n",
        "\n",
        "# 중복을 제거한 단어들의 집합인 단어 집합 생성.\n",
        "word_set = set(train_data.split()) \n",
        "\n",
        "# 단어 집합의 각 단어에 고유한 정수 맵핑\n",
        "vocab = {word: i+2 for i, word in enumerate(word_set)} # 미등록어와 패딩을 위한 인덱스 부여\n",
        "vocab['unk'] = 0\n",
        "vocab['pad'] = 1\n",
        "print(vocab)\n",
        "\n",
        "# 임베딩 테이블: (단어수, 임베딩 사이즈)\n",
        "# Word2Vec, GloVe, Random\n",
        "# 단어 수 만큼 lookup table을 만듬 - 세팅해서 tensor로 만듬\n",
        "embedding_table = torch.FloatTensor([[ 0.0,  0.0,  0.0],\n",
        "                                     [ 0.0,  0.0,  0.0],\n",
        "                                     [ 0.1,  0.8,  0.3],\n",
        "                                     [ 0.7,  0.8,  0.2],\n",
        "                                     [ 0.1,  0.8,  0.7],\n",
        "                                     [ 0.9,  0.2,  0.1],\n",
        "                                     [ 0.1,  0.1,  0.9],\n",
        "                                     [ 0.2,  0.1,  0.7],\n",
        "                                     [ 0.3,  0.1,  0.1]])\n",
        "\n",
        "# 입력 문장\n",
        "input_snt = 'I like football'.split()\n",
        "\n",
        "# 각 단어를 정수로 변환\n",
        "idxes=[]\n",
        "\n",
        "for word in input_snt:\n",
        "  idx = vocab[word] if word in vocab else vocab['unk']\n",
        "  idxes.append(idx)\n",
        "\n",
        "idxes = torch.LongTensor(idxes) #인덱스를 tensor로 바꿈\n",
        "\n",
        "# 입력 문장의임베딩가져오기: ['I', 'like', 'football (unk)']\n",
        "lookup_result = embedding_table[idxes, :] # 3개의 단어 -> 인덱스로 가져옴\n",
        "print(lookup_result)\n",
        "\n",
        "# 임베딩 층 만들기\n",
        "embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=3, padding_idx=1) #backpropagation하면서 업데이트\n",
        "print(embedding_layer.weight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'enjoy': 2, 'like': 3, 'learning': 4, 'flying': 5, 'deep': 6, 'I': 7, 'NLP': 8, 'unk': 0, 'pad': 1}\n",
            "tensor([[0.2000, 0.1000, 0.7000],\n",
            "        [0.7000, 0.8000, 0.2000],\n",
            "        [0.0000, 0.0000, 0.0000]])\n",
            "Parameter containing:\n",
            "tensor([[-0.1289, -0.8329, -0.6357],\n",
            "        [ 0.0000,  0.0000,  0.0000],\n",
            "        [ 0.3707,  0.9090,  0.5469],\n",
            "        [ 0.6152, -0.2019,  0.4137],\n",
            "        [ 1.9100,  0.2001,  0.6432],\n",
            "        [-1.5340,  0.2546, -0.1211],\n",
            "        [-1.1414,  0.1044, -0.4543],\n",
            "        [-1.6581,  0.0898,  1.7386],\n",
            "        [-0.6676,  0.4591, -0.1105]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X2Ksgb6jMvn",
        "outputId": "7d517987-0ec4-469d-fed8-1f40353758e8"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 6.7MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYcRu50-lfIe",
        "outputId": "2dc38e7b-ff69-4a31-81d7-e0d9a0fae8d6"
      },
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "input_file = \"/gdrive/MyDrive/colab/text_rep/corpus.txt\"\n",
        "vocab_size = 500\n",
        "model_name = 'subword_kor'\n",
        "model_type = 'bpe'\n",
        "user_defined_symbols = '[PAD],[UNK],[CLS],[SEP],[MASK],[UNK1],[UNK2],[UNK3],[UNK4],[UNK5]'\n",
        "\n",
        "input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --user_defined_symbols=%s --model_type=%s'\n",
        "cmd = input_argument%(input_file, model_name, vocab_size,user_defined_symbols, model_type)\n",
        "\n",
        "spm.SentencePieceTrainer.Train(cmd)\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"subword_kor.model\")\n",
        "wp = sp.EncodeAsPieces(\"하늘을 우러러\")\n",
        "id = sp.EncodeAsIds(\"하늘을 우러러\")\n",
        "print(wp)\n",
        "print(id)\n",
        "\n",
        "wp2snt= sp.DecodePieces(wp)\n",
        "id2snt = sp.DecodeIds(id)\n",
        "print(wp2snt)\n",
        "print(id2snt)\n",
        "\n",
        "print(\"Vocab. Size =\",sp.GetPieceSize())\n",
        "print(sp.IdToPiece(58))\n",
        "print(sp.PieceToId('하'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁', '하', '늘', '을', '▁우', '러', '러']\n",
            "[174, 193, 356, 186, 80, 315, 315]\n",
            "하늘을 우러러\n",
            "하늘을 우러러\n",
            "Vocab. Size = 500\n",
            "▁강\n",
            "193\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}